digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2458905222800 [label="
 (128, 10)" fillcolor=darkolivegreen1]
	2458906210272 [label=AddmmBackward0]
	2458906212288 -> 2458906210272
	2458906300864 [label="classifier.fc.bias
 (10)" fillcolor=lightblue]
	2458906300864 -> 2458906212288
	2458906212288 [label=AccumulateGrad]
	2458906212192 -> 2458906210272
	2458906212192 [label=MulBackward0]
	2458906213056 -> 2458906212192
	2458906213056 [label=ViewBackward0]
	2458906213200 -> 2458906213056
	2458906213200 [label=ReluBackward0]
	2458906214016 -> 2458906213200
	2458906214016 [label=NativeBatchNormBackward0]
	2458906213968 -> 2458906214016
	2458906213968 [label=ConvolutionBackward0]
	2458906210800 -> 2458906213968
	2458906210800 [label=ReluBackward0]
	2458906211568 -> 2458906210800
	2458906211568 [label=ConvolutionBackward0]
	2458906211472 -> 2458906211568
	2458906211472 [label=MaxPool2DWithIndicesBackward0]
	2458906212672 -> 2458906211472
	2458906212672 [label=ReluBackward0]
	2458906207536 -> 2458906212672
	2458906207536 [label=NativeBatchNormBackward0]
	2458906210944 -> 2458906207536
	2458906210944 [label=ConvolutionBackward0]
	2458906211136 -> 2458906210944
	2458906211136 [label=MaxPool2DWithIndicesBackward0]
	2458906213776 -> 2458906211136
	2458906213776 [label=ReluBackward0]
	2458906214352 -> 2458906213776
	2458906214352 [label=NativeBatchNormBackward0]
	2458906212768 -> 2458906214352
	2458906212768 [label=ConvolutionBackward0]
	2458906210560 -> 2458906212768
	2458906210560 [label=ReluBackward0]
	2458906213296 -> 2458906210560
	2458906213296 [label=ConvolutionBackward0]
	2458906213488 -> 2458906213296
	2458906690080 [label="features.conv1.weight
 (16, 3, 3, 3)" fillcolor=lightblue]
	2458906690080 -> 2458906213488
	2458906213488 [label=AccumulateGrad]
	2458906213536 -> 2458906213296
	2458906690160 [label="features.conv1.bias
 (16)" fillcolor=lightblue]
	2458906690160 -> 2458906213536
	2458906213536 [label=AccumulateGrad]
	2458906213872 -> 2458906212768
	2460213947552 [label="features.conv2.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	2460213947552 -> 2458906213872
	2458906213872 [label=AccumulateGrad]
	2458906213920 -> 2458906212768
	2460440779792 [label="features.conv2.bias
 (32)" fillcolor=lightblue]
	2460440779792 -> 2458906213920
	2458906213920 [label=AccumulateGrad]
	2458906211184 -> 2458906214352
	2458905512592 [label="features.batchNorm1.weight
 (32)" fillcolor=lightblue]
	2458905512592 -> 2458906211184
	2458906211184 [label=AccumulateGrad]
	2458906212912 -> 2458906214352
	2460365055344 [label="features.batchNorm1.bias
 (32)" fillcolor=lightblue]
	2460365055344 -> 2458906212912
	2458906212912 [label=AccumulateGrad]
	2458906211088 -> 2458906210944
	2460461534640 [label="features.conv3.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	2460461534640 -> 2458906211088
	2458906211088 [label=AccumulateGrad]
	2458906211040 -> 2458906210944
	2460461532240 [label="features.conv3.bias
 (64)" fillcolor=lightblue]
	2460461532240 -> 2458906211040
	2458906211040 [label=AccumulateGrad]
	2458906210896 -> 2458906207536
	2458903918800 [label="features.batchNorm2.weight
 (64)" fillcolor=lightblue]
	2458903918800 -> 2458906210896
	2458906210896 [label=AccumulateGrad]
	2458906211376 -> 2458906207536
	2458906690320 [label="features.batchNorm2.bias
 (64)" fillcolor=lightblue]
	2458906690320 -> 2458906211376
	2458906211376 [label=AccumulateGrad]
	2458906211520 -> 2458906211568
	2458906690800 [label="features.conv4.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2458906690800 -> 2458906211520
	2458906211520 [label=AccumulateGrad]
	2458906209888 -> 2458906211568
	2458906690880 [label="features.conv4.bias
 (128)" fillcolor=lightblue]
	2458906690880 -> 2458906209888
	2458906209888 [label=AccumulateGrad]
	2458906210848 -> 2458906213968
	2458906691040 [label="features.conv5.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2458906691040 -> 2458906210848
	2458906210848 [label=AccumulateGrad]
	2458906210368 -> 2458906213968
	2458906691120 [label="features.conv5.bias
 (256)" fillcolor=lightblue]
	2458906691120 -> 2458906210368
	2458906210368 [label=AccumulateGrad]
	2458906214304 -> 2458906214016
	2458906691200 [label="features.batchNorm3.weight
 (256)" fillcolor=lightblue]
	2458906691200 -> 2458906214304
	2458906214304 [label=AccumulateGrad]
	2458906213008 -> 2458906214016
	2458906691280 [label="features.batchNorm3.bias
 (256)" fillcolor=lightblue]
	2458906691280 -> 2458906213008
	2458906213008 [label=AccumulateGrad]
	2458906212240 -> 2458906210272
	2458906212240 [label=TBackward0]
	2458906214064 -> 2458906212240
	2460461534000 [label="classifier.fc.weight
 (10, 16384)" fillcolor=lightblue]
	2460461534000 -> 2458906214064
	2458906214064 [label=AccumulateGrad]
	2458906210272 -> 2458905222800
}
